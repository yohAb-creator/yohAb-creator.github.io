[{"id":0,"href":"/projects/","title":"Research Projects","section":"Welcome","content":"Welcome to the project archive. Choose a project from the sidebar to dive into the methodology, results, and open questions. New write-ups will appear here as they are published.\n"},{"id":1,"href":"/projects/topological-prompt-distillation/","title":"Topological Prompt Distillation","section":"Research Projects","content":"Introduction: Asking the Right Question In order to receive an answer, one must first ask a question. This concept also holds when it comes to our interaction with Large Language Models(LLMs)— to receive a desirable answer, we must first provide a prompt that is adequately crafted. The prompt needs to contain just enough information to guide the LLM towards the correct solution, while also avoiding extraneous information that might derail the LLM\u0026rsquo;s answer. Crafting a perfect prompt might be challenging due to various reasons, including a prompt encapsulating some other text. A solution for this problem is to distill existing prompts down to their most critical components, a method referred to as prompt compression.\nMethodology: Journey from Text to Structure Semantic Similarity The first step in the journey involved in changing a prompt into a structured instruction is to filter redundant sentences through the use of semantic similarity. The way I did this was through the use of cosine similarity on the embeddings of the sentences. We start by considering a single sentence embedding and appending it to an empty list. As we iteratively go through the other sentences, we check the similarity between them and the other embeddings contained in the list. Sentence embeddings with cosine similarity less than some threshold value are appended to the list, while embeddings with a similarity score higher than the threshold are discarded.\nSemantic Significance The next step is to prioritize sentences based on semantic significance. As the area I am considering is primarily competitive programming, I first initialize a dictionary composed of keys corresponding to the input, the operation, and the goal. The corresponding values for these keys include a range of terms common in competitive programming. The sentences are assigned scores based on how many relevant terms they contain and the frequency with which the words occur. Then, both the sentences and the sentence embeddings are sorted in their respective lists according to their scores.\nPrompt Distillation with Topological Data Analysis(TDA) Now comes the final leg of the journey. Presented with a prompt, we first split it into separate sentences. The sentences are then embedded into a vector as discussed previously. Since the more similar sentences are, the higher their cosine similarity will be(with a maximum score of 1), we can subtract the cosine similarity from 1 to create a notion of distance—more similar sentences lie closer together. Using this distance as a basis, we create a Vietoris-Rips Complex Filtration and track the first homology group. Then, 1-cycles that last above a certain threshold of \u0026ldquo;time\u0026rdquo; are taken to be persistent cycles. The persistent cycles are then used to group the sentences making up the prompts into clusters based on which sentences belong to which persistent 1-cycles. Constraint sentences are determined through the presence of some common constraint words. In the case of a cluster containing multiple constraint sentences, only one constraint is picked as the others would just be redundant. Once we identify and make sure that the question and input sentences are included, we combine the sentences that passed through the distillation. The combined sentences are then passed through the semantic similarity filter to remove duplicates.\n"},{"id":2,"href":"/projects/tda-augmented-competitive-programming/","title":"TDA Augmented Competitive Programming with LLMs","section":"Research Projects","content":"Abstract A concise summary of the post\u0026rsquo;s content and significance.\nIntroduction Despite their apparent capacity for reasoning, current large language models (LLMs) do not engage in \u0026rsquo;thinking\u0026rsquo; in the conventional cognitive sense. Fundamentally, they are autoregressive probabilistic systems: they generate output by selecting the next token based on statistical correlations with preceding tokens. This mechanism, while capable of producing coherent and contextually rich language, lacks true intentionality, abstract awareness, or internal representation of goals, beliefs, or logical continuity beyond learned statistical patterns.\nMathematical Background The mathematical framework we employ arises from the domain of Algebraic Topology, with particular emphasis on the construct of persistent homology. This tool enables us to extract and quantify topological features across multiple scales, offering a robust means of encoding the underlying structure of high-dimensional data—particularly the evolving landscape of reasoning trajectories within large language models.\nThe Boundary Homomorphism $\\partial_k$ Definition.\n[Group of k-chains] Given an oriented simplicial complex $\\mathcal{K}$, let $\\mathcal{B}_k$ be the set of all $k$-dimensional simplices in $\\mathcal{K}$. The group $C_k$ is defined to be the free abelian group with $\\mathcal{B}_k$ as its basis. A free abelian group F is a group where each element of the basis forms an infinite cyclic subgroup. That is \\[\u0026lt;x\u0026gt; \\simeq Z, \\quad x \\in B\\] where $\\simeq$ expresses an isomorphism, Z is the group of integers under addition, and B is the basis of the free abelian group. Additionally, \\[F= \\sum_{b \\in B} \u0026lt;b\u0026gt;\\] and for $x \\in F$ \\[x = \\sum_{b \\in B} m_b b\\] where $m_b \\in Z$. As such, elements of $C_k$ are finite formal sums referred to as $k$-chains. A typical $k$-chain $c \\in C_k$ is given by, $$ c=\\sum_{i=0}^N a_i \\sigma_i $$ where $a_i \\in \\mathbb{Z}$ and $\\sigma_i$ is an oriented simplex of dimension $k$. Essentially, the group $C_k$ is the collection of all possible $k$-chains. $C_k$ is the group of all possible k-chains. Thus, the elements of $C_k$ are k-chains and not simplicies. The definition for the boundary map should be\nDefinition.\n[Boundary of a \\( k \\)-Simplex] For an oriented \\( k \\)-simplex \\( \\sigma \\in B_k \\), given by the convex hull of affinely independent points \\( (v_0, \\dots, v_k) \\), the boundary of \\( \\sigma \\) is \\[ \\partial_k(\\sigma) = \\sum_{i=0}^{k} (-1)^i (v_0, \\dots, \\hat{v}_i, \\dots, v_k) \\in C_{k-1}, \\] where \\( (v_0, \\dots, \\hat{v}_i, \\dots, v_k) \\) is the oriented \\( (k-1) \\)-simplex given by the convex hull of the points \\( \\{v_0, \\dots, v_{i-1}, v_{i+1}, \\dots, v_k\\} \\), the \\( i \\)-th face of \\( \\sigma \\). Definition.\n[\\( k \\)-th Boundary Map for \\( C_k \\)] For \\( k \\geq 0 \\), the \\( k \\)-th boundary map \\( \\partial_k : C_k \\rightarrow C_{k-1} \\) is the homomorphism defined by linearly extending the boundary of each basis element \\( \\sigma \\in B_k \\): for a \\( k \\)-chain \\( c = \\sum a_i \\sigma_i \\in C_k \\), where \\( \\sigma_i \\in B_k \\) and \\( a_i \\in \\mathbb{Z} \\), \\[ \\partial_k(c) = \\sum a_i \\partial_k(\\sigma_i). \\] Methodology The idea is to take the thoughts generated by the Tree of Thoughts framework and construct a thought space. We employ persistent homology to detect holes and topological features, i.e., 0-holes (connected components), 1-holes (loops), and higher-dimensional holes corresponding to more complex relationships. The next step is to identify persistent features across multiple problem-solving strategies, as these most likely represent essential subtasks conducive to solving the problem\u0026hellip;\n"},{"id":3,"href":"/about/","title":"About Me","section":"Welcome","content":"Hi, I’m Yohannes Physicist and mathematician focused on computational modeling and topological methods.\nI study how structure arises from data—whether in the geometry of high-dimensional embeddings or the dynamics of physical systems.\nMy work bridges theory and implementation, combining machine learning, algebraic topology, and theoretical physics to build models that explain and predict.\nCurrently, I’m exploring how topological tools can extract semantic meaning from language inputs to large models.\nEducation BA in Mathematics and Physics from College of Wooster (2024) Interests Algebraic Topology Quantum Mechanics Multiversal Theory Advanced Computational Methods Non-Euclidean Geometry Contact Email: yohannesabateneh@gmail.com Twitter: @username Google Scholar: Yohan on Google Scholar GitHub: Yohan on GitHub "},{"id":4,"href":"/posts/my-first-research-post/","title":"My First Research Post","section":"Posts","content":"Abstract A concise summary of the post\u0026rsquo;s content and significance.\nIntroduction The context and background for this research exploration.\nTheoretical Framework $$ H\\Psi = E\\Psi $$\nWhere $H$ represents the Hamiltonian operator and $\\Psi$ the wave function.\nMethodology # Example R code for data analysis data \u0026lt;- read.csv(\u0026#34;measurements.csv\u0026#34;) model \u0026lt;- lm(response ~ predictor, data=data) summary(model)"},{"id":5,"href":"/contact/","title":"Contact","section":"Welcome","content":"For professional inquiries, collaborations, or other questions, please feel free to reach out via the methods below.\nEmail yohannesabateneh@gmail.com\nProfessional Profiles GitHub: yohAb-creator Linkedin: Yohannes Abateneh "}]