<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Projects on The Bridge: A Research Focused Blog</title>
    <link>https://yohab-creator.github.io/projects/</link>
    <description>Recent content in Projects on The Bridge: A Research Focused Blog</description>
    <generator>Hugo -- 0.147.0</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 18:42:14 -0500</lastBuildDate>
    <atom:link href="https://yohab-creator.github.io/projects/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Topological Prompt Distillation</title>
      <link>https://yohab-creator.github.io/projects/topological_distillation/</link>
      <pubDate>Tue, 10 Jun 2025 18:42:14 -0500</pubDate>
      <guid>https://yohab-creator.github.io/projects/topological_distillation/</guid>
      <description>&lt;h1 id=&#34;introduction-asking-the-right-question&#34;&gt;Introduction: Asking the Right Question&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;In order to receive an answer, one must first ask a question. This concept also holds when it comes to our interaction with Large Language Models(LLMs)â€” to receive a desirable answer, we must first provide a prompt that is adequately crafted. The prompt needs to contain just enough information to guide the LLM towards the correct solution, while also avoiding extraneous information that might derail the LLM&amp;rsquo;s answer.
Crafting a perfect prompt might be challenging due to various reasons, including a prompt encapsulating some other text. A solution for this problem is to distill existing prompts down to their most critical components, a method referred to as prompt compression.&lt;/p&gt;</description>
    </item>
    <item>
      <title>TDA Augmented Competitive Programming with LLMs</title>
      <link>https://yohab-creator.github.io/projects/competitive_programming_with_llms/</link>
      <pubDate>Wed, 21 May 2025 16:35:02 -0500</pubDate>
      <guid>https://yohab-creator.github.io/projects/competitive_programming_with_llms/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;A concise summary of the post&amp;rsquo;s content and significance.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Despite their apparent capacity for reasoning, current large language models (LLMs) do not engage in &amp;rsquo;thinking&amp;rsquo; in the conventional cognitive sense. Fundamentally, they are autoregressive probabilistic systems: they generate output by selecting the next token based on statistical correlations with preceding tokens. This mechanism, while capable of producing coherent and contextually rich language, lacks true intentionality, abstract awareness, or internal representation of goals, beliefs, or logical continuity beyond learned statistical patterns.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
