<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Topological Data Analysis on The Bridge: A Research Focused Blog</title>
    <link>https://yohab-creator.github.io/tags/topological-data-analysis/</link>
    <description>Recent content in Topological Data Analysis on The Bridge: A Research Focused Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 18:42:14 -0500</lastBuildDate>
    <atom:link href="https://yohab-creator.github.io/tags/topological-data-analysis/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Topological Prompt Distillation</title>
      <link>https://yohab-creator.github.io/projects/topological-prompt-distillation/</link>
      <pubDate>Tue, 10 Jun 2025 18:42:14 -0500</pubDate>
      <guid>https://yohab-creator.github.io/projects/topological-prompt-distillation/</guid>
      <description>&lt;h1 id=&#34;introduction-asking-the-right-question&#34;&gt;Introduction: Asking the Right Question&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;In order to receive an answer, one must first ask a question. This concept also holds when it comes to our interaction with Large Language Models(LLMs)â€” to receive a desirable answer, we must first provide a prompt that is adequately crafted. The prompt needs to contain just enough information to guide the LLM towards the correct solution, while also avoiding extraneous information that might derail the LLM&amp;rsquo;s answer.&#xA;Crafting a perfect prompt might be challenging due to various reasons, including a prompt encapsulating some other text. A solution for this problem is to distill existing prompts down to their most critical components, a method referred to as prompt compression.&lt;/p&gt;</description>
    </item>
    <item>
      <title>TDA Augmented Competitive Programming with LLMs</title>
      <link>https://yohab-creator.github.io/projects/tda-augmented-competitive-programming/</link>
      <pubDate>Wed, 21 May 2025 16:35:02 -0500</pubDate>
      <guid>https://yohab-creator.github.io/projects/tda-augmented-competitive-programming/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;&#xA;&lt;p&gt;A concise summary of the post&amp;rsquo;s content and significance.&lt;/p&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Despite their apparent capacity for reasoning, current large language models (LLMs) do not engage in &amp;rsquo;thinking&amp;rsquo; in the conventional cognitive sense. Fundamentally, they are autoregressive probabilistic systems: they generate output by selecting the next token based on statistical correlations with preceding tokens. This mechanism, while capable of producing coherent and contextually rich language, lacks true intentionality, abstract awareness, or internal representation of goals, beliefs, or logical continuity beyond learned statistical patterns.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Topological Tree Pruning</title>
      <link>https://yohab-creator.github.io/projects/topological-tree-pruning/</link>
      <pubDate>Thu, 05 Dec 2024 18:42:14 -0500</pubDate>
      <guid>https://yohab-creator.github.io/projects/topological-tree-pruning/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;The tree of thoughts framework is a powerful method.&lt;/p&gt;&#xA;&lt;h1 id=&#34;methodology&#34;&gt;Methodology:&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;semantic-similarity&#34;&gt;Semantic Similarity&lt;/h2&gt;&#xA;&lt;p&gt;The first step in the journey involved in changing a prompt into a structured instruction is to filter redundant sentences through the use of semantic similarity. The way I did this was through the use of cosine similarity on the embeddings of the sentences. We start by considering a single sentence embedding and appending it to an empty list. As we iteratively go through the other sentences, we check the similarity between them and the other embeddings contained in the list. Sentence embeddings with cosine similarity less than some threshold value are appended to the list, while embeddings with a similarity score higher than the threshold are discarded.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
